{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more interesting data augmentation we need to extract the actual data instead of using the sha256 hash.\n",
    "# Because of this we will be using ember's functions to extract their json like objects to analyse the data in full.|\n",
    "\n",
    "# Be sure you have the ember2018 data set downloaded and unzipped. \n",
    "\n",
    "#Imports for opening dataset:\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emberpath = r\"C:/Users/Spider/PycharmProjects/embertest\\ember2018\"\n",
    "def find_file(filename,base_path):\n",
    "    \"\"\"\n",
    "    Check folder and inner folders for all files and if base named file is found store location in variable and return\n",
    "    Returns first found matching item location so be sure you specify what you need and in the correct path.\n",
    "    \"\"\"\n",
    "    foundfile = \"\"\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            #Check folder for all files and if base named file is found store location in variable and return:\n",
    "            if filename in file:\n",
    "                foundfile = os.path.join(root,file)\n",
    "                return foundfile\n",
    "    return \"\"\n",
    "\n",
    "#Get file path\n",
    "basefile = find_file(\"train_features_1.jsonl\",emberpath)\n",
    "secondfile = find_file(\"train_features_2.jsonl\",emberpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded to df\n",
      "loaded to json\n",
      "catagorized df\n",
      "do same thing for dataset 2 and 15 minutes more.\n",
      "loaded to df\n",
      "loaded to json\n",
      "catagorized df\n"
     ]
    }
   ],
   "source": [
    "def load_json_lines_df(file):\n",
    "    with open(basefile) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    df_inter = pd.DataFrame(lines)\n",
    "    print(\"loaded to df\")\n",
    "    df_inter.columns = ['json_element']\n",
    "    df_inter['json_element'].apply(json.loads)\n",
    "    print(\"loaded to json\")\n",
    "    df_final = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "    print(\"catagorized df\")\n",
    "    return df_final\n",
    "\n",
    "#load file into a dataframe for analysis. *TAKES A WHILE! 7 minutes on my pc?* AND! This requires atleast 6gb of RAM.\n",
    "df = load_json_lines_df(basefile)\n",
    "print(\"do same thing for dataset 2 and 15 minutes more.\")\n",
    "df2 = load_json_lines_df(secondfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256\n",
      "md5\n",
      "appeared\n",
      "label\n",
      "avclass\n",
      "histogram\n",
      "byteentropy\n",
      "exports\n",
      "datadirectories\n",
      "strings.numstrings\n",
      "strings.avlength\n",
      "strings.printabledist\n",
      "strings.printables\n",
      "strings.entropy\n",
      "strings.paths\n",
      "strings.urls\n",
      "strings.registry\n",
      "strings.MZ\n",
      "general.size\n",
      "general.vsize\n",
      "general.has_debug\n",
      "general.exports\n",
      "general.imports\n",
      "general.has_relocations\n",
      "general.has_resources\n",
      "general.has_signature\n",
      "general.has_tls\n",
      "general.symbols\n",
      "header.coff.timestamp\n",
      "header.coff.machine\n",
      "header.coff.characteristics\n",
      "header.optional.subsystem\n",
      "header.optional.dll_characteristics\n",
      "header.optional.magic\n",
      "header.optional.major_image_version\n",
      "header.optional.minor_image_version\n",
      "header.optional.major_linker_version\n",
      "header.optional.minor_linker_version\n",
      "header.optional.major_operating_system_version\n",
      "header.optional.minor_operating_system_version\n",
      "header.optional.major_subsystem_version\n",
      "header.optional.minor_subsystem_version\n",
      "header.optional.sizeof_code\n",
      "header.optional.sizeof_headers\n",
      "header.optional.sizeof_heap_commit\n",
      "section.entry\n",
      "section.sections\n"
     ]
    }
   ],
   "source": [
    "for item in list(df.columns):\n",
    "    #print all usefull information.\n",
    "    if item[0:7] not in \"imports\":\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(list(df.filter(regex = 'import*')), axis = 1, inplace = True)\n",
    "df2.drop(list(df.filter(regex = 'import*')), axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sha256', 'md5', 'appeared', 'label', 'avclass', 'histogram',\n",
       "       'byteentropy', 'exports', 'datadirectories', 'strings.numstrings',\n",
       "       'strings.avlength', 'strings.printabledist', 'strings.printables',\n",
       "       'strings.entropy', 'strings.paths', 'strings.urls', 'strings.registry',\n",
       "       'strings.MZ', 'general.size', 'general.vsize', 'general.has_debug',\n",
       "       'general.exports', 'general.has_relocations', 'general.has_resources',\n",
       "       'general.has_signature', 'general.has_tls', 'general.symbols',\n",
       "       'header.coff.timestamp', 'header.coff.machine',\n",
       "       'header.coff.characteristics', 'header.optional.subsystem',\n",
       "       'header.optional.dll_characteristics', 'header.optional.magic',\n",
       "       'header.optional.major_image_version',\n",
       "       'header.optional.minor_image_version',\n",
       "       'header.optional.major_linker_version',\n",
       "       'header.optional.minor_linker_version',\n",
       "       'header.optional.major_operating_system_version',\n",
       "       'header.optional.minor_operating_system_version',\n",
       "       'header.optional.major_subsystem_version',\n",
       "       'header.optional.minor_subsystem_version',\n",
       "       'header.optional.sizeof_code', 'header.optional.sizeof_headers',\n",
       "       'header.optional.sizeof_heap_commit', 'section.entry',\n",
       "       'section.sections'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#smaller cleaned dataset by removing all dll's for loadtimes reasons.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export data set after pruning:\n",
    "df.to_csv(\"pruneddataset3.csv\")\n",
    "df2.to_csv(\"pruneddataset4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
